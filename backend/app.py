# backend/app.py
"""
Bç«™è§†é¢‘ä¿¡æ¯çˆ¬å–ã€ä¸‹è½½ä¸AIæ€»ç»“å·¥å…· - åç«¯æœåŠ¡
"""
import os
import subprocess
import json
import threading
import traceback
import re
import time
import random
from datetime import datetime
from flask import Flask, request, jsonify, send_file, send_from_directory
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
from openai import OpenAI

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from crawler import BilibiliCrawler
from transcriber import get_transcriber, TranscriptResult

# ========== é…ç½® ==========
os.environ['PATH'] = '/opt/homebrew/bin:/usr/local/bin:' + os.environ.get('PATH', '')

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
FRONTEND_DIR = os.path.join(BASE_DIR, 'frontend')
DOWNLOAD_DIR = os.path.join(BASE_DIR, 'downloads')
UPLOAD_DIR = os.path.join(BASE_DIR, 'uploads')

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__, static_folder=FRONTEND_DIR, static_url_path='')
CORS(app, supports_credentials=True, resources={r"/*": {"origins": "*"}})

app.config['UPLOAD_FOLDER'] = UPLOAD_DIR
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB
app.config['ALLOWED_EXTENSIONS'] = {'xlsx', 'xls'}

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DOWNLOAD_DIR, exist_ok=True)
os.makedirs(UPLOAD_DIR, exist_ok=True)

# ========== å…¨å±€çŠ¶æ€å­˜å‚¨ ==========
# ä¸‹è½½ä»»åŠ¡çŠ¶æ€
download_task_status = {}

# è½¬å†™ä»»åŠ¡çŠ¶æ€
transcribe_status = {}

# çˆ¬è™«ä»»åŠ¡çŠ¶æ€
crawler_status = {
    'is_running': False,
    'is_paused': False,
    'progress': 0,
    'current_task': '',
    'total_keywords': 0,
    'processed_keywords': 0,
    'total_videos': 0,
    'current_keyword': '',
    'error': None,
    'logs': [],
    'videos': []
}

# åˆå§‹åŒ–çˆ¬è™«
crawler = BilibiliCrawler()


# ========== å·¥å…·å‡½æ•° ==========
def allowed_file(filename):
    return '.' in filename and \
        filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']


def add_crawler_log(message, is_error=False):
    """æ·»åŠ çˆ¬è™«æ—¥å¿—"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    log_entry = {
        'timestamp': timestamp,
        'message': message,
        'is_error': is_error
    }
    crawler_status['logs'].append(log_entry)
    if len(crawler_status['logs']) > 100:
        crawler_status['logs'] = crawler_status['logs'][-100:]


def read_keywords(filepath):
    """è¯»å–å…³é”®è¯Excelæ–‡ä»¶"""
    try:
        df = pd.read_excel(filepath)
        keywords = df['item'].tolist()
        return [str(keyword).strip() for keyword in keywords if pd.notna(keyword)]
    except Exception as e:
        print(f"è¯»å–å…³é”®è¯æ–‡ä»¶å¤±è´¥: {e}")
        return []


# ========== çˆ¬è™«ä»»åŠ¡ ==========
def run_crawler_task(filename, pages_per_keyword=5, enable_detailed_info=True, remove_duplicates=True):
    """è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    global crawler_status

    try:
        crawler_status['is_running'] = True
        crawler_status['is_paused'] = False
        crawler_status['progress'] = 0
        crawler_status['error'] = None
        crawler_status['logs'] = []
        crawler_status['videos'] = []

        # è¯»å–å…³é”®è¯
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        keywords = read_keywords(filepath)
        if not keywords:
            crawler_status['error'] = "æœªæ‰¾åˆ°å…³é”®è¯"
            add_crawler_log("æœªæ‰¾åˆ°å…³é”®è¯", True)
            return

        crawler_status['total_keywords'] = len(keywords)
        crawler_status['processed_keywords'] = 0
        crawler_status['total_videos'] = 0

        add_crawler_log(f"æ‰¾åˆ° {len(keywords)} ä¸ªå…³é”®è¯")

        all_videos = []

        # ç¬¬ä¸€é˜¶æ®µï¼šæœç´¢å¹¶æŠ“å–åŸºç¡€ä¿¡æ¯
        for i, keyword in enumerate(keywords):
            while crawler_status['is_paused']:
                if not crawler_status['is_running']:
                    return
                time.sleep(1)

            if not crawler_status['is_running']:
                add_crawler_log("ä»»åŠ¡å·²åœæ­¢")
                return

            crawler_status['current_keyword'] = keyword
            crawler_status['processed_keywords'] = i
            crawler_status['progress'] = int((i / len(keywords)) * 50)

            add_crawler_log(f"å¼€å§‹å¤„ç†å…³é”®è¯: {keyword}")

            for page in range(1, pages_per_keyword + 1):
                add_crawler_log(f"å¤„ç†ç¬¬{page}é¡µ...")

                videos = crawler.search(keyword, page)

                if videos:
                    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    for video in videos:
                        video['æœç´¢å…³é”®è¯'] = keyword
                        video['æ“ä½œæ—¶é—´'] = current_time

                    all_videos.extend(videos)
                    crawler_status['total_videos'] = len(all_videos)
                    crawler_status['videos'] = all_videos
                    add_crawler_log(f"ç¬¬{page}é¡µæˆåŠŸè·å–åˆ° {len(videos)} ä¸ªè§†é¢‘")
                else:
                    add_crawler_log(f"ç¬¬{page}é¡µæœªè·å–åˆ°æ•°æ®")

                delay = random.uniform(3, 5)
                add_crawler_log(f"ç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)

            keyword_count = len([v for v in all_videos if v['æœç´¢å…³é”®è¯'] == keyword])
            add_crawler_log(f"å…³é”®è¯ '{keyword}' å¤„ç†å®Œæˆï¼Œå…±è·å– {keyword_count} ä¸ªè§†é¢‘")

        # ç¬¬äºŒé˜¶æ®µï¼šè¡¥å……è¯¦ç»†ä¿¡æ¯
        if all_videos:
            crawler_status['progress'] = 50
            crawler_status['current_task'] = 'æ­£åœ¨è¡¥å……è§†é¢‘è¯¦ç»†ä¿¡æ¯...'
            add_crawler_log("å¼€å§‹è¡¥å……è§†é¢‘è¯¦ç»†ä¿¡æ¯...")

            # å»é‡
            if remove_duplicates:
                df_temp = pd.DataFrame(all_videos)
                before_count = len(df_temp)
                df_temp = df_temp.drop_duplicates(subset=['bvid'], keep='first')
                after_count = len(df_temp)

                if before_count != after_count:
                    add_crawler_log(f"å»é™¤äº† {before_count - after_count} ä¸ªé‡å¤è§†é¢‘")

                all_videos = df_temp.to_dict('records')

            # è¡¥å……è¯¦ç»†ä¿¡æ¯
            if enable_detailed_info:
                enriched_videos = crawler.enrich_videos(all_videos,
                                                        progress_callback=lambda msg: add_crawler_log(msg))
            else:
                enriched_videos = all_videos

            # ç¬¬ä¸‰é˜¶æ®µï¼šä¿å­˜æ•°æ®
            crawler_status['progress'] = 90
            crawler_status['current_task'] = 'æ­£åœ¨ä¿å­˜æ•°æ®...'
            add_crawler_log("å¼€å§‹ä¿å­˜æ•°æ®...")

            df = pd.DataFrame(enriched_videos)

            columns_order = [
                'bvid', 'title', 'arcurl', 'description', 'author',
                'uploadDate', 'play', 'review', 'tag', 'pubdate',
                'duration', 'æœç´¢å…³é”®è¯', 'æ“ä½œæ—¶é—´'
            ]

            for col in columns_order:
                if col not in df.columns:
                    df[col] = ''

            df = df[columns_order]

            output_filename = os.path.join(DOWNLOAD_DIR, 'BVID.xlsx')
            df.to_excel(output_filename, index=False)

            crawler_status['progress'] = 100
            crawler_status['current_task'] = 'ä»»åŠ¡å®Œæˆï¼'
            crawler_status['videos'] = enriched_videos
            add_crawler_log(f"æ•°æ®å·²ä¿å­˜åˆ° {output_filename}")
            add_crawler_log(f"æ€»å…±è·å–åˆ° {len(df)} ä¸ªå”¯ä¸€è§†é¢‘æ•°æ®")
        else:
            crawler_status['error'] = "æœªè·å–åˆ°ä»»ä½•æ•°æ®"
            add_crawler_log("æœªè·å–åˆ°ä»»ä½•æ•°æ®", True)

    except Exception as e:
        crawler_status['error'] = f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {str(e)}"
        add_crawler_log(f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {str(e)}", True)
    finally:
        crawler_status['is_running'] = False
        crawler_status['is_paused'] = False

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if filename.startswith('temp_keywords_'):
            try:
                temp_filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                if os.path.exists(temp_filepath):
                    os.remove(temp_filepath)
            except:
                pass


# ========== ä¸‹è½½ä»»åŠ¡ ==========
def run_yt_dlp(bvid, download_type, task_id):
    """è¿è¡Œyt-dlpä¸‹è½½"""
    url = f"https://www.bilibili.com/video/{bvid}"
    output_dir = os.path.join(DOWNLOAD_DIR, bvid)
    os.makedirs(output_dir, exist_ok=True)

    download_task_status[task_id] = {"status": "downloading", "progress": 0, "message": "å¼€å§‹ä¸‹è½½..."}

    try:
        import shutil
        ffmpeg_path = shutil.which('ffmpeg')
        ffmpeg_dir = os.path.dirname(ffmpeg_path) if ffmpeg_path else '/opt/homebrew/bin'

        base_cmd = [
            "yt-dlp",
            "--no-warnings",
            "--newline",
            "--ffmpeg-location", ffmpeg_dir,
        ]

        if download_type == "audio":
            cmd = base_cmd + [
                "-f", "bestaudio[ext=m4a]/bestaudio",
                "-o", os.path.join(output_dir, "%(title)s.%(ext)s"),
                url
            ]
        elif download_type == "video_only":
            cmd = base_cmd + [
                "-f", "bestvideo[ext=mp4]/bestvideo",
                "-o", os.path.join(output_dir, "%(title)s_video.%(ext)s"),
                url
            ]
        elif download_type == "danmaku":
            cmd = base_cmd + [
                "--write-subs", "--sub-langs", "danmaku",
                "--skip-download",
                "-o", os.path.join(output_dir, "%(title)s"),
                url
            ]
        else:  # merged
            cmd = base_cmd + [
                "-f", "bestvideo[ext=mp4]+bestaudio[ext=m4a]/bestvideo+bestaudio/best",
                "--merge-output-format", "mp4",
                "-o", os.path.join(output_dir, "%(title)s.%(ext)s"),
                url
            ]

        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            env=os.environ
        )

        output_lines = []
        for line in process.stdout:
            line = line.strip()
            if not line:
                continue
            output_lines.append(line)
            download_task_status[task_id]["message"] = line[:100]

            if '%' in line:
                try:
                    match = re.search(r'(\d+\.?\d*)%', line)
                    if match:
                        progress = float(match.group(1))
                        download_task_status[task_id]["progress"] = min(progress, 99)
                except:
                    pass

        process.wait()

        if process.returncode == 0:
            files = os.listdir(output_dir) if os.path.exists(output_dir) else []
            if files:
                download_task_status[task_id] = {
                    "status": "completed",
                    "progress": 100,
                    "message": f"ä¸‹è½½å®Œæˆï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶",
                    "output_dir": output_dir
                }
            else:
                download_task_status[task_id] = {
                    "status": "error",
                    "message": "ä¸‹è½½å®Œæˆä½†æœªæ‰¾åˆ°æ–‡ä»¶"
                }
        else:
            error_lines = [l for l in output_lines if 'error' in l.lower()]
            error_msg = error_lines[-1] if error_lines else output_lines[-1] if output_lines else "æœªçŸ¥é”™è¯¯"
            download_task_status[task_id] = {
                "status": "error",
                "message": error_msg[:200]
            }

    except FileNotFoundError:
        download_task_status[task_id] = {
            "status": "error",
            "message": "yt-dlp æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install yt-dlp"
        }
    except Exception as e:
        download_task_status[task_id] = {
            "status": "error",
            "message": f"å¼‚å¸¸: {str(e)}"
        }


# ========== è½¬å†™ä»»åŠ¡ ==========
def run_transcribe(bvid, audio_file, task_id, output_formats):
    """åå°è¿è¡Œè½¬å†™ä»»åŠ¡"""
    try:
        output_dir = os.path.join(DOWNLOAD_DIR, bvid)

        def progress_callback(message, progress):
            transcribe_status[task_id] = {
                "status": "transcribing",
                "progress": progress,
                "message": message
            }

        transcriber = get_transcriber(model_size="medium")
        transcriber.set_progress_callback(progress_callback)

        output = transcriber.transcribe_and_save(
            audio_file,
            output_dir,
            formats=output_formats,
            language="zh"
        )

        result: TranscriptResult = output["result"]

        transcribe_status[task_id] = {
            "status": "completed",
            "progress": 100,
            "message": "è½¬å†™å®Œæˆ",
            "text": result.text,
            "timestamped_text": result.to_timestamped_text(),
            "segments": [
                {
                    "start": seg.start,
                    "end": seg.end,
                    "start_formatted": seg.start_formatted,
                    "end_formatted": seg.end_formatted,
                    "text": seg.text
                }
                for seg in result.segments
            ],
            "duration": result.duration,
            "language": result.language,
            "files": output["files"]
        }

    except Exception as e:
        transcribe_status[task_id] = {
            "status": "error",
            "progress": 0,
            "message": f"è½¬å†™å¤±è´¥: {str(e)}"
        }


# ========== å‰ç«¯è·¯ç”± ==========
@app.route('/')
def index():
    return send_from_directory(FRONTEND_DIR, 'index.html')


@app.route('/<path:filename>')
def serve_static(filename):
    return send_from_directory(FRONTEND_DIR, filename)


# ========== çˆ¬è™« API ==========
@app.route('/api/crawler/upload', methods=['POST'])
def crawler_upload_file():
    """ä¸Šä¼ å…³é”®è¯æ–‡ä»¶å¹¶å¼€å§‹çˆ¬å–"""
    if 'file' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400

    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)

        pages = request.form.get('pages', 5, type=int)
        enable_detailed_info = request.form.get('enable_detailed_info', 'true') == 'true'
        remove_duplicates = request.form.get('remove_duplicates', 'true') == 'true'

        thread = threading.Thread(
            target=run_crawler_task,
            args=(filename, pages, enable_detailed_info, remove_duplicates)
        )
        thread.daemon = True
        thread.start()

        keywords = read_keywords(filepath)

        return jsonify({
            'message': 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹çˆ¬å–æ•°æ®',
            'filename': filename,
            'keywords_count': len(keywords),
            'keywords': keywords
        })

    return jsonify({'error': 'æ–‡ä»¶ç±»å‹ä¸æ”¯æŒ'}), 400


@app.route('/api/crawler/start-with-keywords', methods=['POST'])
def crawler_start_with_keywords():
    """ä½¿ç”¨æ‰‹åŠ¨è¾“å…¥çš„å…³é”®è¯å¼€å§‹çˆ¬å–"""
    try:
        keywords_json = request.form.get('keywords')
        if not keywords_json:
            return jsonify({'error': 'æ²¡æœ‰æä¾›å…³é”®è¯'}), 400

        keywords = json.loads(keywords_json)
        if not keywords or not isinstance(keywords, list):
            return jsonify({'error': 'å…³é”®è¯æ ¼å¼ä¸æ­£ç¡®'}), 400

        pages = request.form.get('pages', 5, type=int)
        enable_detailed_info = request.form.get('enable_detailed_info', 'true') == 'true'
        remove_duplicates = request.form.get('remove_duplicates', 'true') == 'true'

        temp_filename = f"temp_keywords_{int(time.time())}.xlsx"
        temp_filepath = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)

        df = pd.DataFrame({'item': keywords})
        df.to_excel(temp_filepath, index=False)

        thread = threading.Thread(
            target=run_crawler_task,
            args=(temp_filename, pages, enable_detailed_info, remove_duplicates)
        )
        thread.daemon = True
        thread.start()

        return jsonify({
            'message': 'å¼€å§‹çˆ¬å–æ•°æ®',
            'keywords_count': len(keywords),
            'keywords': keywords
        })

    except Exception as e:
        return jsonify({'error': f'å¤„ç†å…³é”®è¯å¤±è´¥: {str(e)}'}), 500


@app.route('/api/crawler/status')
def crawler_get_status():
    """è·å–çˆ¬è™«çŠ¶æ€"""
    return jsonify(crawler_status)


@app.route('/api/crawler/pause', methods=['POST'])
def crawler_pause():
    """æš‚åœçˆ¬è™«"""
    global crawler_status
    if crawler_status['is_running'] and not crawler_status['is_paused']:
        crawler_status['is_paused'] = True
        add_crawler_log("ä»»åŠ¡å·²æš‚åœ")
    return jsonify({'message': 'ä»»åŠ¡å·²æš‚åœ'})


@app.route('/api/crawler/resume', methods=['POST'])
def crawler_resume():
    """ç»§ç»­çˆ¬è™«"""
    global crawler_status
    if crawler_status['is_running'] and crawler_status['is_paused']:
        crawler_status['is_paused'] = False
        add_crawler_log("ä»»åŠ¡ç»§ç»­æ‰§è¡Œ")
    return jsonify({'message': 'ä»»åŠ¡ç»§ç»­æ‰§è¡Œ'})


@app.route('/api/crawler/stop', methods=['POST'])
def crawler_stop():
    """åœæ­¢çˆ¬è™«"""
    global crawler_status
    crawler_status['is_running'] = False
    crawler_status['is_paused'] = False
    crawler_status['current_task'] = 'ä»»åŠ¡å·²åœæ­¢'
    add_crawler_log("ä»»åŠ¡å·²åœæ­¢")
    return jsonify({'message': 'ä»»åŠ¡å·²åœæ­¢'})


@app.route('/api/crawler/download')
def crawler_download():
    """ä¸‹è½½çˆ¬å–ç»“æœ"""
    filepath = os.path.join(DOWNLOAD_DIR, 'BVID.xlsx')
    if os.path.exists(filepath):
        return send_file(filepath, as_attachment=True, download_name='BVID.xlsx')
    else:
        return jsonify({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404


# ========== ä¸‹è½½ API ==========
@app.route('/api/download', methods=['POST'])
def download_video():
    """å¯åŠ¨ä¸‹è½½ä»»åŠ¡"""
    data = request.json
    bvids = data.get('bvids', [])
    download_type = data.get('type', 'merged')

    task_ids = []
    for bvid in bvids:
        bvid = bvid.strip()
        if not bvid:
            continue
        if 'bilibili.com' in bvid:
            match = re.search(r'(BV[\w]+)', bvid)
            if match:
                bvid = match.group(1)

        task_id = f"{bvid}_{download_type}"
        task_ids.append(task_id)

        thread = threading.Thread(target=run_yt_dlp, args=(bvid, download_type, task_id))
        thread.daemon = True
        thread.start()

    return jsonify({"task_ids": task_ids})


@app.route('/api/download/status/<task_id>', methods=['GET'])
def get_download_status(task_id):
    """è·å–ä¸‹è½½ä»»åŠ¡çŠ¶æ€"""
    status = download_task_status.get(task_id, {"status": "unknown", "message": "ä»»åŠ¡ä¸å­˜åœ¨"})
    return jsonify(status)


@app.route('/api/downloads', methods=['GET'])
def list_downloads():
    """åˆ—å‡ºæ‰€æœ‰å·²ä¸‹è½½çš„å†…å®¹"""
    downloads = []

    if not os.path.exists(DOWNLOAD_DIR):
        return jsonify({"downloads": []})

    for bvid in os.listdir(DOWNLOAD_DIR):
        bvid_dir = os.path.join(DOWNLOAD_DIR, bvid)
        if not os.path.isdir(bvid_dir):
            continue

        files = []
        has_audio = False
        has_video = False
        has_transcript = False
        title = bvid

        for f in os.listdir(bvid_dir):
            filepath = os.path.join(bvid_dir, f)
            if not os.path.isfile(filepath):
                continue

            file_info = {
                "name": f,
                "size": os.path.getsize(filepath),
                "path": filepath
            }
            files.append(file_info)

            ext = os.path.splitext(f)[1].lower()
            if ext in ['.m4a', '.mp3', '.wav', '.aac']:
                has_audio = True
                title = os.path.splitext(f)[0]
            elif ext in ['.mp4', '.webm', '.flv', '.mkv']:
                has_video = True
                if not has_audio:
                    title = os.path.splitext(f)[0].replace('_video', '')
            elif ext == '.txt' and not f.endswith('_timestamped.txt'):
                has_transcript = True
            elif ext in ['.srt', '.json']:
                has_transcript = True

        if files:
            downloads.append({
                "bvid": bvid,
                "title": title,
                "files": files,
                "has_audio": has_audio,
                "has_video": has_video,
                "has_transcript": has_transcript,
                "file_count": len(files),
                "total_size": sum(f["size"] for f in files)
            })

    downloads.sort(key=lambda x: max(
        os.path.getmtime(f["path"]) for f in x["files"]
    ) if x["files"] else 0, reverse=True)

    return jsonify({"downloads": downloads})


@app.route('/api/files/<bvid>', methods=['GET'])
def list_files(bvid):
    """åˆ—å‡ºä¸‹è½½çš„æ–‡ä»¶"""
    output_dir = os.path.join(DOWNLOAD_DIR, bvid)
    if not os.path.exists(output_dir):
        return jsonify({"files": []})

    files = []
    for f in os.listdir(output_dir):
        filepath = os.path.join(output_dir, f)
        if os.path.isfile(filepath):
            files.append({
                "name": f,
                "size": os.path.getsize(filepath),
                "path": filepath
            })

    return jsonify({"files": files})


@app.route('/api/delete/<bvid>', methods=['DELETE'])
def delete_download(bvid):
    """åˆ é™¤ä¸‹è½½çš„å†…å®¹"""
    import shutil
    output_dir = os.path.join(DOWNLOAD_DIR, bvid)

    if not os.path.exists(output_dir):
        return jsonify({"error": "ç›®å½•ä¸å­˜åœ¨"}), 404

    try:
        shutil.rmtree(output_dir)
        return jsonify({"success": True, "message": f"å·²åˆ é™¤ {bvid}"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ========== è½¬å†™ API ==========
@app.route('/api/transcribe', methods=['POST'])
def transcribe_audio():
    """å¯åŠ¨éŸ³é¢‘è½¬æ–‡æœ¬ä»»åŠ¡"""
    data = request.json
    bvid = data.get('bvid')
    output_formats = data.get('formats', ['txt'])

    output_dir = os.path.join(DOWNLOAD_DIR, bvid)

    if not os.path.exists(output_dir):
        return jsonify({"error": f"ç›®å½•ä¸å­˜åœ¨: {bvid}ï¼Œè¯·å…ˆä¸‹è½½è§†é¢‘"}), 404

    audio_file = None
    for f in os.listdir(output_dir):
        if f.endswith(('.m4a', '.mp3', '.wav', '.mp4', '.webm', '.flv', '.aac')):
            audio_file = os.path.join(output_dir, f)
            break

    if not audio_file:
        return jsonify({"error": "æœªæ‰¾åˆ°éŸ³é¢‘/è§†é¢‘æ–‡ä»¶"}), 404

    task_id = f"transcribe_{bvid}"

    if task_id in transcribe_status and transcribe_status[task_id]["status"] == "completed":
        return jsonify({
            "task_id": task_id,
            "status": "completed",
            "cached": True,
            **transcribe_status[task_id]
        })

    transcribe_status[task_id] = {
        "status": "starting",
        "progress": 0,
        "message": "æ­£åœ¨å¯åŠ¨è½¬å†™ä»»åŠ¡..."
    }

    thread = threading.Thread(
        target=run_transcribe,
        args=(bvid, audio_file, task_id, output_formats)
    )
    thread.daemon = True
    thread.start()

    return jsonify({"task_id": task_id, "status": "started"})

@app.route('/api/transcript/<bvid>', methods=['GET'])
def get_transcript_content(bvid):
    """è·å–è½¬å†™æ–‡æœ¬å†…å®¹"""
    output_dir = os.path.join(DOWNLOAD_DIR, bvid)

    if not os.path.exists(output_dir):
        return jsonify({"error": "ç›®å½•ä¸å­˜åœ¨"}), 404

    # æŸ¥æ‰¾txtæ–‡ä»¶
    transcript_text = None
    timestamped_text = None

    for f in os.listdir(output_dir):
        filepath = os.path.join(output_dir, f)
        if f.endswith('.txt') and not f.endswith('_timestamped.txt'):
            try:
                with open(filepath, 'r', encoding='utf-8') as file:
                    transcript_text = file.read()
            except:
                pass
        elif f.endswith('_timestamped.txt'):
            try:
                with open(filepath, 'r', encoding='utf-8') as file:
                    timestamped_text = file.read()
            except:
                pass

    if transcript_text is None:
        return jsonify({"error": "æœªæ‰¾åˆ°è½¬å†™æ–‡ä»¶"}), 404

    return jsonify({
        "text": transcript_text,
        "timestamped_text": timestamped_text
    })

@app.route('/api/transcribe/status/<task_id>', methods=['GET'])
def get_transcribe_status(task_id):
    """è·å–è½¬å†™ä»»åŠ¡çŠ¶æ€"""
    status = transcribe_status.get(task_id, {"status": "unknown", "message": "ä»»åŠ¡ä¸å­˜åœ¨"})
    return jsonify(status)


# ========== AIæ€»ç»“ API ==========
@app.route('/api/summarize', methods=['POST'])
def summarize_text():
    """è°ƒç”¨APIæ€»ç»“æ–‡æœ¬"""
    data = request.json
    text = data.get('text', '')
    base_url = data.get('base_url', 'https://api.openai.com/v1')
    api_key = data.get('api_key', '')
    prompt = data.get('prompt', 'è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹çš„ä¸»è¦è§‚ç‚¹ï¼š')
    model = data.get('model', 'gpt-3.5-turbo')

    if not api_key:
        return jsonify({"error": "è¯·æä¾›API Key"}), 400

    if not text:
        return jsonify({"error": "è¯·æä¾›è¦æ€»ç»“çš„æ–‡æœ¬"}), 400

    try:
        client = OpenAI(base_url=base_url, api_key=api_key)

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ€»ç»“åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": f"{prompt}\n\n{text}"}
            ]
        )

        summary = response.choices[0].message.content
        return jsonify({"summary": summary})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ========== å¯åŠ¨æœåŠ¡å™¨ ==========
if __name__ == '__main__':
    print(f"\n{'=' * 60}")
    print("ğŸ¬ Bç«™è§†é¢‘ä¿¡æ¯çˆ¬å–ã€ä¸‹è½½ä¸AIæ€»ç»“å·¥å…·")
    print(f"{'=' * 60}")
    print(f"ğŸ“ Frontend: {FRONTEND_DIR}")
    print(f"ğŸ“ Downloads: {DOWNLOAD_DIR}")
    print(f"ğŸ“ Uploads: {UPLOAD_DIR}")
    print(f"\nğŸŒ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://localhost:5000")
    print(f"{'=' * 60}\n")

    app.run(debug=True, port=5000, threaded=True)


